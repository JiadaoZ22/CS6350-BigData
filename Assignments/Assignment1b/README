It is a full README including assignment1
--------------------------------------------------------
--------------------------------------------------------
--------------------------------------------------------
# Assignment 1b
Contributor: Jiadao Zou jxz172230

1. upload ass1-1.0.jar to SSH under your account

2. Problem 2 (download the material we may use in the future):
	To run part 2,
	> hadoop jar ass1-1.0.jar ass1_2.ass1_2 topic
	It will create a folder <topic-search> under your hdfs root directory.
	> hdfs dfs -ls <topic-search>


	for example:
		topic => economy
		time:  "2019-2-1",
                "2019-2-2",
                "2019-2-3",
                "2019-2-4",
                "2019-2-5",
                "2019-2-6",
        size:	for convenience, I only query 500 results for each day, which makes the 6 files around 3.4M in total, However, it is possible to query as much as possible as long as change one parameter.

--------------------------------------------------------
--------------------------------------------------------
--------------------------------------------------------
# Assignment 1b
Contributor: Jiadao Zou jxz172230

Acknowledgement:

1) The project use Maven to manage its dependencies, to compile the entire project, use IDEA's maven managing module's package button>

2) To run the program, place the ass1b.jar to Hadoop cluster and
follow the commands below.

3) create the working directory
> hdfs dfs -mkdir -p assignment1b

# create easy access
> HW1b=hdfs://cshadoop1/user/jxz172230/assignment1b

--------------------------------------------------------
Part 1 Word Count

# Cleaning output directory
> hdfs dfs -rm -r $HW1b/*

# upload stopwords.txt to HDFS
> hdfs dfs -put stopwords.txt $HW1b

# Count words from text files stored on HDFS
> hadoop jar ass1b.jar \
  ass1b1.TopWordCount \
  <topic-search> \
  $HW1b/1out \
  $HW1b/stopwords.txt


# Verify Results
> hdfs dfs -ls $HW1b/1out
# Display the result
> hdfs dfs -cat $HW1b/1out/part-r-00000

# Cleanup
> unset HW1b

--------------------------------------------------------
Part 2 Movie Rating

According to README.html from movielens.org, the rating data file structure is as described below:

### Ratings Data File Structure (ratings.csv)
All ratings are contained in the file ratings.csv. Each line of this file after the header row represents one rating of one movie by one user, and has the following format:

userId,movieId,rating,timestamp
The lines within this file are ordered first by userId, then, within user, by movieId.

Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).

Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.

# Setup
> HW1b=hdfs://cshadoop1/user/jxz172230/assignment1b

# Run the movie rating hadoop job
> hadoop jar ass1b.jar \
  ass1b2.MovieRating \
  hdfs://cshadoop1/movielens/ratings.csv \
  $HW1b/2out

# Verify Results
> hdfs dfs -ls $HW1b/partii/out

rm -f movieRatingOutput
hdfs dfs -cat $HW1b/2out/part-r-00000

--------------------------------------------------------
Part 3 Get the result

> rm -f wordcountOutput.txt
> hdfs dfs -get $HW1b/1out/part-r-00000 wordcountOutput.txt

> rm -f movieRatingOutput.txt
> hdfs dfs -get $HW1b/2out/part-r-00000 movieRatingOutput.txt

# cleanup quick access
> unset HW1b

My results are within the folder:results